{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTji8EqLvwiI"
      },
      "outputs": [],
      "source": [
        "# we will be using various libraries like os for taking the input,etc\n",
        "# I have used some libraries like seaborn, wordcloud, matplotlib for data visualization so\n",
        "# you can skip them if you don't understand\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "\n",
        "# re is used for cleaning the dataset\n",
        "\n",
        "import re\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow import keras\n",
        "\n",
        "# callbacks are important here as sometimes you get the best accuracy earlies and then it\n",
        "# goes down so as to stop the training there you need to use them\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding,Conv1D,LSTM,GRU,BatchNormalization,Flatten,Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FC7cguHFwfs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('/content/drive/MyDrive/DS102_Đồ án cuối kì/cr7-cr7-1-cr7-cr7-1-cr7-cr7-1-cr7-cr7-1.csv')\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "UrQS_3OTwCoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "HOGgeQoMxFSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "7PU0YRKDxIJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=df['sentiment'])\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "eQBvwhpjxJug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=df['vi_review']\n",
        "le=LabelEncoder()\n",
        "df['sentiment']= le.fit_transform(df['sentiment'])"
      ],
      "metadata": {
        "id": "LaohkJD4xMQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment']"
      ],
      "metadata": {
        "id": "1MKtbW-gHuvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "pos=' '.join(map(str,sentences[df['sentiment']==2]))\n",
        "neg=' '.join(map(str,sentences[df['sentiment']==0]))\n",
        "\n",
        "wordcloud1 = WordCloud(width = 800, height = 800,\n",
        "                background_color ='black',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10).generate(pos)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(wordcloud1)\n",
        "plt.title('Positive Sentiment')\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "bPs4oCYHxdNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "wordcloud2 = WordCloud(width = 800, height = 800,\n",
        "                background_color ='black',\n",
        "                stopwords = stopwords,\n",
        "                min_font_size = 10).generate(neg)\n",
        "\n",
        "plt.imshow(wordcloud2)\n",
        "plt.title('Negative Sentiment')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zD8LRBe8xpGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neutral = ' '.join(map(str, sentences[df['sentiment'] == 1]))\n",
        "wordcloud2 = WordCloud(width=800, height=800,\n",
        "                       background_color='black',\n",
        "                       stopwords=stopwords,\n",
        "                       min_font_size=10).generate(neutral)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud2)\n",
        "plt.title('Neutral Sentiment')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Svo3PPl3ySP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = df['sentiment'].nunique()\n",
        "labels = to_categorical(df['sentiment'], num_classes=num_classes)\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(df['vi_review'],labels,test_size=0.1,random_state=10)\n"
      ],
      "metadata": {
        "id": "btRihaeVyZLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings= np.load('/content/drive/MyDrive/DS102_Đồ án cuối kì/glove.840B.300d.pkl',\n",
        "                          allow_pickle=True)"
      ],
      "metadata": {
        "id": "ESuYwjB04ePP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vocab_build(review):\n",
        "\n",
        "    comments = review.apply(lambda s: s.split()).values #Chia câu thành các từ và chuyển sang thành numpy\n",
        "    vocab={}\n",
        "#Duyệt qua từng đánh giá và từng từ trong đánh giá , cập nhật từ vựng.\n",
        "    for comment in comments:\n",
        "        for word in comment:\n",
        "            try:\n",
        "                vocab[word]+=1\n",
        "\n",
        "            except KeyError:\n",
        "                vocab[word]=1\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "ugvejxHRz1yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_coverage(review,embeddings):\n",
        "\n",
        "    vocab=vocab_build(review)\n",
        "\n",
        "    covered={}\n",
        "    word_count={}\n",
        "    oov={}\n",
        "    covered_num=0\n",
        "    oov_num=0\n",
        "\n",
        "    for word in vocab:\n",
        "        try:\n",
        "            covered[word]=embeddings[word]\n",
        "            covered_num+=vocab[word]\n",
        "            word_count[word]=vocab[word]\n",
        "        except:\n",
        "            oov[word]=vocab[word]\n",
        "            oov_num+=oov[word]\n",
        "\n",
        "    vocab_coverage=len(covered)/len(vocab)*100\n",
        "    text_coverage = covered_num/(covered_num+oov_num)*100\n",
        "\n",
        "    sorted_oov=sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
        "    sorted_word_count=sorted(word_count.items(), key=operator.itemgetter(1))[::-1]\n",
        "\n",
        "    return sorted_word_count,sorted_oov,vocab_coverage,text_coverage"
      ],
      "metadata": {
        "id": "7w8ylXGY0W_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\n",
        "test_covered,test_oov, test_vocab_coverage, test_text_coverage = embedding_coverage(X_test,glove_embeddings)\n",
        "\n",
        "print(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\n",
        "print(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in testing set\")"
      ],
      "metadata": {
        "id": "qFEyQxr70W8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_oov[:20]"
      ],
      "metadata": {
        "id": "xgIYGbc70k0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "3j25gTdUESUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n",
        "def clean_sentences(line):\n",
        "\n",
        "    line=re.sub('<.*?>','',line) # removing html tags\n",
        "    #special characters and emojis\n",
        "    line=re.sub('\\x91The','The',line)\n",
        "    line=re.sub('\\x97','',line)\n",
        "    line=re.sub('\\x84The','The',line)\n",
        "    line=re.sub('\\uf0b7','',line)\n",
        "    line=re.sub('¡¨','',line)\n",
        "    line=re.sub('\\x95','',line)\n",
        "    line=re.sub('\\x8ei\\x9eek','',line)\n",
        "    line=re.sub('\\xad','',line)\n",
        "    line=re.sub('\\x84bubble','bubble',line)\n",
        "\n",
        "    # remove concated words\n",
        "    line=re.sub('trivialBoring','trivial Boring',line)\n",
        "    line=re.sub('Justforkix','Just for kix',line)\n",
        "    line=re.sub('Nightbeast','Night beast',line)\n",
        "    line=re.sub('DEATHTRAP','Death Trap',line)\n",
        "    line=re.sub('CitizenX','Citizen X',line)\n",
        "    line=re.sub('10Rated','10 Rated',line)\n",
        "    line=re.sub('_The','_ The',line)\n",
        "    line=re.sub('1Sound','1 Sound',line)\n",
        "    line=re.sub('blahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblahblah','blah blah',line)\n",
        "    line=re.sub('ResidentHazard','Resident Hazard',line)\n",
        "    line=re.sub('iameracing','i am racing',line)\n",
        "    line=re.sub('BLACKSNAKE','Black Snake',line)\n",
        "    line=re.sub('DEATHSTALKER','Death Stalker',line)\n",
        "    line=re.sub('_is_','is',line)\n",
        "    line=re.sub('10Fans','10 Fans',line)\n",
        "    line=re.sub('Yellowcoat','Yellow coat',line)\n",
        "    line=re.sub('Spiderbabe','Spider babe',line)\n",
        "    line=re.sub('Frightworld','Fright world',line)\n",
        "\n",
        "    #removing punctuations\n",
        "    punctuations = '@#!~?+&*[]-%._-:/£();$=><|{}^' + '''\"“´”'`'''\n",
        "    for p in punctuations:\n",
        "        line = line.replace(p, f' {p} ')\n",
        "\n",
        "    line=re.sub(',',' , ',line)\n",
        "\n",
        "    # ... and ..\n",
        "    line = line.replace('...', ' ... ')\n",
        "\n",
        "    if '...' not in line:\n",
        "        line = line.replace('..', ' ... ')\n",
        "\n",
        "    return line"
      ],
      "metadata": {
        "id": "gELgEpcU52AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X_train.apply(lambda s: clean_sentences(s))\n",
        "X_test=X_test.apply(lambda s: clean_sentences(s))\n",
        "\n",
        "train_covered,train_oov,train_vocab_coverage,train_text_coverage=embedding_coverage(X_train,glove_embeddings)\n",
        "print(f\"Glove embeddings cover {round(train_vocab_coverage,2)}% of vocabulary and {round(train_text_coverage,2)}% text in training set\")\n",
        "\n",
        "test_covered,test_oov,test_vocab_coverage,test_text_coverage=embedding_coverage(X_test,glove_embeddings)\n",
        "print(f\"Glove embeddings cover {round(test_vocab_coverage,2)}% of vocabulary and {round(test_text_coverage,2)}% text in testing set\")"
      ],
      "metadata": {
        "id": "VqVduCjq6PPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuations = '@#!~?+&*[]-%._-:/£();$=><|{},^' + '''\"“´”'`'''\n",
        "train_word=[]\n",
        "train_count=[]\n",
        "\n",
        "i=1\n",
        "for word,count in train_covered:\n",
        "    if word not in punctuations:\n",
        "        train_word.append(word)\n",
        "        train_count.append(count)\n",
        "        i+=1\n",
        "    if(i==15):\n",
        "        break"
      ],
      "metadata": {
        "id": "h3Pcwz986QTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_word=[]\n",
        "test_count=[]\n",
        "\n",
        "i=1\n",
        "for word,count in test_covered:\n",
        "    if word not in punctuations:\n",
        "        test_word.append(word)\n",
        "        test_count.append(count)\n",
        "        i+=1\n",
        "    if(i==15):\n",
        "        break\n"
      ],
      "metadata": {
        "id": "BVZXoL7d6Ts2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=train_count,y=train_word).set_title('Count of 15 most used word in training set')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "zWbqFvK26WWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x=test_count,y=test_word).set_title('Count of 15 most used word in testing set')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "nG__PoeT6Xjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del glove_embeddings,train_oov,test_oov\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "lwFTR-Jr6ac_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words=80000\n",
        "embeddings=256"
      ],
      "metadata": {
        "id": "HfjfSL2Y6ci7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(num_words=num_words,oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index=tokenizer.word_index\n",
        "total_vocab=len(word_index)"
      ],
      "metadata": {
        "id": "bQpYuuPm6eYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary of the dataset is : \",total_vocab)"
      ],
      "metadata": {
        "id": "GzGB0Yn16fti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_train=tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test=tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "max_len=max(max([len(x) for x in sequences_train]),max([len(x) for x in sequences_test]))\n",
        "\n",
        "train_padded=pad_sequences(sequences_train,maxlen=max_len)\n",
        "test_padded=pad_sequences(sequences_test,maxlen=max_len)"
      ],
      "metadata": {
        "id": "etYBLa-f6iwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_val,Y_train,Y_val=train_test_split(train_padded,Y_train,\n",
        "                                             test_size=0.05,random_state=10)"
      ],
      "metadata": {
        "id": "CpBtV3Pl6kab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= keras.Sequential()\n",
        "model.add(Embedding(num_words,embeddings,input_length=max_len))\n",
        "model.add(Conv1D(256,10,activation='relu'))\n",
        "model.add(keras.layers.Bidirectional(LSTM(128,return_sequences=True)))\n",
        "model.add(LSTM(64))\n",
        "model.add(keras.layers.Dropout(0.4))\n",
        "model.add(Dense(3,activation='softmax'))"
      ],
      "metadata": {
        "id": "P02eLuDq6mBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PpTas2AI6ncB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']\n",
        "             )"
      ],
      "metadata": {
        "id": "HFIArfV46pwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es= EarlyStopping(monitor='val_accuracy',\n",
        "                  patience=3\n",
        "                 )\n",
        "\n",
        "checkpoints=ModelCheckpoint(filepath='/content/drive/MyDrive',\n",
        "                            monitor=\"val_accuracy\",\n",
        "                            verbose=0,\n",
        "                            save_best_only=True\n",
        "                           )\n",
        "\n",
        "callbacks=[es,checkpoints]"
      ],
      "metadata": {
        "id": "mNuqOXqY6qXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit(X_train,Y_train,validation_data=(X_val,Y_val),epochs=5,callbacks=callbacks)"
      ],
      "metadata": {
        "id": "IcxNq9VW6tAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graph(history,string):\n",
        "\n",
        "    plt.plot(history.history[string],label='training '+string)\n",
        "    plt.plot(history.history['val_'+string],label='validation '+string)\n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel(string)\n",
        "    plt.title(string+' vs epochs')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lW8FnG-97GlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(history,'loss')"
      ],
      "metadata": {
        "id": "P5nyn6gL7KKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_graph(history,'accuracy')"
      ],
      "metadata": {
        "id": "ejgIVKDt7SZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "while True:\n",
        "    # Người dùng nhập câu\n",
        "    user_input = input(\"Nhập câu của bạn (hoặc nhập 'exit' để thoát): \")\n",
        "\n",
        "    # Kiểm tra điều kiện để thoát vòng lặp\n",
        "    if user_input.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    # Tiền xử lý câu và đưa vào mô hình dự đoán\n",
        "    cleaned_input = clean_sentences(user_input)\n",
        "    tokenized_input = tokenizer.texts_to_sequences([cleaned_input])\n",
        "    padded_input = pad_sequences(tokenized_input, maxlen=max_len)\n",
        "\n",
        "    # Dự đoán\n",
        "    predictions = model.predict(padded_input)\n",
        "    sentiment_labels = ['Negative', 'Neutral', 'Positive']\n",
        "    predicted_sentiment_index = np.argmax(predictions, axis=1)[0]\n",
        "    predicted_sentiment = sentiment_labels[predicted_sentiment_index]\n",
        "\n",
        "    # In kết quả\n",
        "    print(f\"Máy dự đoán: {predicted_sentiment}\")\n"
      ],
      "metadata": {
        "id": "5bXsCruNHMEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN\n"
      ],
      "metadata": {
        "id": "vKVkynb-h0vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Khởi tạo danh sách lưu trữ độ chính xác\n",
        "score_list = []\n",
        "\n",
        "# Lặp qua các giá trị k từ 1 đến 14\n",
        "for k in range(1, 15):\n",
        "    # Tạo mô hình KNN với số láng giềng là k\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    # Huấn luyện mô hình trên dữ liệu huấn luyện\n",
        "    knn_model.fit(X_train, Y_train)\n",
        "\n",
        "    # Dự đoán nhãn trên tập kiểm thử\n",
        "    y_pred = knn_model.predict(X_val)\n",
        "\n",
        "    # Tính độ chính xác và thêm vào danh sách\n",
        "    accuracy = accuracy_score(Y_val, y_pred)\n",
        "    score_list.append(accuracy)\n",
        "\n",
        "# Vẽ đồ thị\n",
        "plt.plot(range(1, 15), score_list)\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"KNN Accuracy for Different Values of k\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "onUkVI1bft3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, Y_train)\n",
        "Y_val_single_dim = np.argmax(Y_val, axis=1)\n",
        "y_pred = knn.predict(X_val)\n",
        "y_pred_single_dim = np.argmax(y_pred, axis=1)\n",
        "# Evaluate the model using accuracy_score\n",
        "accuracy = accuracy_score(Y_val_single_dim, y_pred_single_dim)\n",
        "print(\"Độ chính xác với Số láng giềng = 3: {:.2f}%\".format(accuracy * 100))"
      ],
      "metadata": {
        "id": "oRQ0nVeyeu4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "metadata": {
        "id": "PtJ8OXylhQHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and display the confusion matrix\n",
        "conf_mat = confusion_matrix(Y_val_single_dim, y_pred_single_dim)\n",
        "f, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(conf_mat, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "plt.xlabel(\"Dự đoán\")\n",
        "plt.ylabel(\"Thực tế\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WhJ58S4gg8IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Tính toán precision, recall, F1-score\n",
        "precision = precision_score(Y_val_single_dim, y_pred_single_dim, average='weighted')\n",
        "recall = recall_score(Y_val_single_dim, y_pred_single_dim, average='weighted')\n",
        "f1 = f1_score(Y_val_single_dim, y_pred_single_dim, average='weighted')\n",
        "\n",
        "# In ra các thông số\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-score: {f1:.2f}')\n",
        "\n",
        "# In ra classification report, bao gồm precision, recall, F1-score và support\n",
        "print('Classification Report:')\n",
        "print(classification_report(Y_val_single_dim, y_pred_single_dim))"
      ],
      "metadata": {
        "id": "M5p7EJaQhbff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM"
      ],
      "metadata": {
        "id": "VLH1goODh3L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method_names = []\n",
        "method_scores = []\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "svm = SVC(random_state=42)\n",
        "svm.fit(X_train, np.argmax(Y_train, axis=1))  # Chuyển đổi one-hot encoding về dạng số nguyên\n",
        "svm_score = svm.score(X_val, np.argmax(Y_val, axis=1))\n",
        "print(\"SVM Classification Score is: {}\".format(svm_score))"
      ],
      "metadata": {
        "id": "dKerKqUAjZTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "y_pred = svm.predict(X_val)\n",
        "conf_mat = confusion_matrix(np.argmax(Y_val, axis=1), y_pred)\n",
        "# Visualization Confusion Matrix\n",
        "f, ax = plt.subplots(figsize=(5, 5))\n",
        "sns.heatmap(conf_mat, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"True Values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cDd33f7ujcFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Calculate F1 score, recall, and precision\n",
        "classification_rep = classification_report(np.argmax(Y_val, axis=1), y_pred)\n",
        "\n",
        "# Print and append to the lists\n",
        "print(\"Classification Report for SVM:\\n\", classification_rep)\n",
        "\n",
        "# Extract individual metrics\n",
        "f1_score = float(classification_rep.split()[-4])\n",
        "recall = float(classification_rep.split()[-3])\n",
        "precision = float(classification_rep.split()[-2])\n",
        "\n",
        "# Append to the lists\n",
        "method_names.extend([\"F1 Score\", \"Recall\", \"Precision\"])\n",
        "method_scores.extend([f1_score, recall, precision])"
      ],
      "metadata": {
        "id": "o63cbPSejmVe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}