{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgdkjVf_oB8j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06058c52-ea78-41f9-e62e-44ebd324cd0b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaYxfGHveC2e"
      },
      "source": [
        "## Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tmjbD8IWI0u"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/adu (1).csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmvowkcWV49I"
      },
      "source": [
        "#ULTIMATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNgDTB69Odxj"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otueGdYQUDYV"
      },
      "source": [
        "Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(txt):\n",
        "    return re.sub(r\"http\\S+\", \"\", txt)"
      ],
      "metadata": {
        "id": "cCPGowHu2AHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x5-4pmiUEjk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bde4b149-def8-42a4-f45e-2d3548713ebf"
      },
      "source": [
        "\"\"\"\n",
        "    Start section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey\n",
        "    V√≠ d·ª•: th·ªßy = thuyr, t∆∞·ª£ng = tuwowngj\n",
        "\"\"\"\n",
        "bang_nguyen_am = [['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],\n",
        "                  ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],\n",
        "                  ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],\n",
        "                  ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e'],\n",
        "                  ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],\n",
        "                  ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i'],\n",
        "                  ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o'],\n",
        "                  ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],\n",
        "                  ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],\n",
        "                  ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u'],\n",
        "                  ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],\n",
        "                  ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "\n",
        "def vn_word_to_telex_type(word):\n",
        "    dau_cau = 0\n",
        "    new_word = ''\n",
        "    for char in word:\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            new_word += char\n",
        "            continue\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "        new_word += bang_nguyen_am[x][-1]\n",
        "    new_word += bang_ky_tu_dau[dau_cau]\n",
        "    return new_word\n",
        "\n",
        "\n",
        "def vn_sentence_to_telex_type(sentence):\n",
        "    \"\"\"\n",
        "    Chuy·ªÉn c√¢u ti·∫øng vi·ªát c√≥ d·∫•u v·ªÅ ki·ªÉu g√µ telex.\n",
        "    :param sentence:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        words[index] = vn_word_to_telex_type(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    End section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    End section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ ki·ªÉu g√µ telex khi kh√¥ng b·∫≠t Unikey\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vn_word_to_telex_type('g√†')"
      ],
      "metadata": {
        "id": "R8fRLySjpbAd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ec5901d7-85a1-4077-d36d-0a8833eeb789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gaf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNG-qc3tRF3Z"
      },
      "source": [
        "# **Chu·∫©n ho√° unicode**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr_EHfVoRUex"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import regex as re\n",
        "\n",
        "uniChars = \"√†√°·∫£√£·∫°√¢·∫ß·∫•·∫©·∫´·∫≠ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªáƒë√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµ√Ä√Å·∫¢√É·∫†√Ç·∫¶·∫§·∫®·∫™·∫¨ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜƒê√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥√ÇƒÇƒê√î∆†∆Ø\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "\n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = '√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'.split(\n",
        "        '|')\n",
        "    charutf8 = \"√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥',\n",
        "        lambda x: dicchar[x.group()], txt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8BMFB0kUNsu"
      },
      "source": [
        "D√πng √≤a √∫y thay o√† u√Ω"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjJPkrKgUOwH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0d1c88-4e97-4c99-fdc3-e9487fea4c95"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "    Start section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ c√°ch g√µ d·∫•u ki·ªÉu c≈©: d√πng √≤a √∫y thay o√† u√Ω\n",
        "    Xem t·∫°i ƒë√¢y:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # √™, ∆°\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuy·ªÉn c√¢u ti·∫øng vi·ªát v·ªÅ chu·∫©n g√µ d·∫•u ki·ªÉu c≈©.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    End section: Chuy·ªÉn c√¢u vƒÉn v·ªÅ c√°ch g√µ d·∫•u ki·ªÉu c≈©: d√πng √≤a √∫y thay o√† u√Ω\n",
        "    Xem t·∫°i ƒë√¢y: https://vi.wikipedia.org/wiki/Quy_t·∫Øc_ƒë·∫∑t_d·∫•u_thanh_trong_ch·ªØ_qu·ªëc_ng·ªØ\n",
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "    print(chuan_hoa_dau_cau_tieng_viet('anh ho√†, ƒëang l√†m.. g√¨au'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anh h√≤a, ƒëang l√†m.. gi√†u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7gXpXMuVnY_"
      },
      "source": [
        "Lo·∫°i b·ªè k√Ω t·ª± c·ªë t√≠nh vi·∫øt d√†i trong c√¢u:\n",
        "\n",
        "eg: ƒë√¢yyyyyyyyyy -> ƒë√¢y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOKJRUGXXiwU"
      },
      "source": [
        "# l·∫•y d·ªØ li·ªáu cho teencode\n",
        "import pandas as pd\n",
        "teencode_df = pd.read_csv('/content/drive/MyDrive/Textual data/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
        "teencode_list = teencode_df['teencode'].to_list()\n",
        "map_list = teencode_df['map'].to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzP1ndUSOiyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1e81c4-3324-4985-c1af-75ce1df99282"
      },
      "source": [
        "# Nltk for connect sent\n",
        "import nltk\n",
        "nltk.download('perluniprops')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Unzipping misc/perluniprops.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YkhCqGL2oB8",
        "outputId": "1ba744ec-4323-4253-a5b1-16d58f63d944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import words\n",
        "\n",
        "english_word_set = set(words.words())\n",
        "\n",
        "def is_english_word(word):\n",
        "    return word.lower() in english_word_set\n",
        "print(is_english_word(\"hello\"))  # True\n",
        "print(is_english_word(\"t·ªôc\"))    # False\n"
      ],
      "metadata": {
        "id": "GBIzKx8sYVQM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b03f3f-4449-42bd-a9dc-a4943d1f9aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtchyroHIsAj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "ffa7b07f-075c-4ecd-9af6-fa7a2000c466"
      },
      "source": [
        "sent = 'hello, cute vcllll t·ªôc ttr∆∞·ªüng ƒëi ƒë√¢u ch∆°i ƒë·∫•yyyyyy. ƒêc cc, ƒë∆∞·ª£c lu√¥n. Hahahahahahaaaaaaaaaaaaaaaa ?'\n",
        "\n",
        "def remove_dub_char(sentence):\n",
        "    sentence = str(sentence)\n",
        "    words = []\n",
        "    for word in sentence.strip().split():\n",
        "        if word in teencode_list:\n",
        "            words.append(word)\n",
        "            continue\n",
        "        if is_english_word(word) == True:\n",
        "            words.append(word)\n",
        "            continue\n",
        "        else: words.append(re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags=re.IGNORECASE))\n",
        "    return ' '.join(words)\n",
        "\n",
        "remove_dub_char(sent)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'helo, cute vcl t·ªôc tr∆∞·ªüng ƒëi ƒë√¢u ch∆°i ƒë·∫•y. ƒêc c, ƒë∆∞·ª£c lu√¥n. Hahahahahaha ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wti4h-oUxSx"
      },
      "source": [
        "T√°ch t·ª´ ti·∫øng Vi·ªát\n",
        "\n",
        "H·ªçc sinh h·ªçc sinh h·ªçc ‚áí H·ªçc_sinh h·ªçc sinh_h·ªçc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7hgkCT_vDpE"
      },
      "source": [
        "link_count = 0\n",
        "unicode_count = 0\n",
        "dau_count = 0\n",
        "lower_count = 0\n",
        "noneed_count = 0\n",
        "space_count = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LOK9e25VuPX"
      },
      "source": [
        "def text_preprocess(document):\n",
        "    global link_count\n",
        "    global unicode_count\n",
        "    global dau_count\n",
        "    global lower_count\n",
        "    global noneed_count\n",
        "    global space_count\n",
        "\n",
        "    document = str(document)\n",
        "\n",
        "    # ƒë∆∞a v·ªÅ lower\n",
        "    ducument_before_lower = document\n",
        "    document = document.lower()\n",
        "    if ducument_before_lower != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_lower)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      lower_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # x√≥a kho·∫£ng tr·∫Øng th·ª´a\n",
        "    ducument_before_space = document\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    if ducument_before_space != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_space)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      space_count += 1\n",
        "    # del document1\n",
        "\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_html)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      link_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # chu·∫©n h√≥a unicode\n",
        "    ducument_before_unicode = document\n",
        "    document = convert_unicode(document)\n",
        "    if ducument_before_unicode != document:\n",
        "      unicode_count += 1\n",
        "    # del document1\n",
        "\n",
        "    # x√≥a c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt\n",
        "    #ducument_before_redundant = document\n",
        "    #document = remove_dub_char(document)\n",
        "    #if ducument_before_redundant != document:\n",
        "      # print(\"Cau chua xu ly:\", ducument_before_redundant)\n",
        "      # print(\"Cau da xu ly:\", document)\n",
        "      #noneed_count += 1\n",
        "    # del document1\n",
        "\n",
        "\n",
        "    # chu·∫©n h√≥a c√°ch g√µ d·∫•u ti·∫øng Vi·ªát\n",
        "    ducument_before_dau = document\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    if ducument_before_dau != document:\n",
        "      print(\"Cau chua xu ly:\", ducument_before_dau)\n",
        "      print(\"Cau da xu ly:\", document)\n",
        "      dau_count += 1\n",
        "    # del document1\n",
        "\n",
        "    return document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPBjgWJtts3t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "afd95d70-422c-47ce-a480-386f0273c309"
      },
      "source": [
        "text_preprocess('Anh th√≠ch ƒÉn c∆°m heheeee')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'anh th√≠ch ƒÉn c∆°m heheeee'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVIIBGL6uUVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e538233-14e1-46ad-fad8-c9f106061a0c"
      },
      "source": [
        "'ho√† thu·∫≠n' == 'h√≤a thu·∫≠n'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYwwNVpAd2PE"
      },
      "source": [
        "#DO IT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqr8Je3NVowX"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/datasetprimary - Sheet1 (2).csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Adfs_BbveIDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f48417d-7dd8-4ec4-efaa-6e180b5c69c6"
      },
      "source": [
        "link_count = 0\n",
        "unicode_count = 0\n",
        "dau_count = 0\n",
        "lower_count = 0\n",
        "noneed_count = 0\n",
        "space_count = 0\n",
        "train['vi_review'] = train['vi_review'].apply(lambda x:text_preprocess(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cau chua xu ly: √™u ∆°i hay qu√°aaaa\n",
            "Cau da xu ly: √™u ∆°i hay qua√°aaa\n",
            "Cau chua xu ly: 21/06/2020\n",
            "Cau da xu ly: 21062020\n",
            "Cau chua xu ly: 12/09 n√®\n",
            "Cau da xu ly: 1209 n√®\n",
            "Cau chua xu ly: thag th·ªß m√¥n ch·∫Øc l√† thag t·ªá nh·∫•t trong lichk s·ª≠ ƒë·ªôi b√≥ng. b·∫Øt tay k√©m ph·∫£n x·∫° ch·∫≠m. ch√¢n l·∫°i c√†ng th·∫£m ho·∫°\n",
            "Cau da xu ly: thag th·ªß m√¥n ch·∫Øc l√† thag t·ªá nh·∫•t trong lichk s·ª≠ ƒë·ªôi b√≥ng. b·∫Øt tay k√©m ph·∫£n x·∫° ch·∫≠m. ch√¢n l·∫°i c√†ng th·∫£m h·ªça\n",
            "Cau chua xu ly: lo·∫°i tai nghe gaming ƒë∆∞·ª£c anh em sƒÉn ƒë√≥n nh·∫•t m√πa h√® n√†y https://shorten.asia/kdvkpxta tham kh·∫£o v√† c√πng t·∫≠n h∆∞·ªüng nh√©\n",
            "Cau da xu ly: lo·∫°i tai nghe gaming ƒë∆∞·ª£c anh em sƒÉn ƒë√≥n nh·∫•t m√πa h√® n√†y https:shorten.asiakdvkpxta tham kh·∫£o v√† c√πng t·∫≠n h∆∞·ªüng nh√©\n",
            "Cau chua xu ly: 8/9/2020 v·∫´n ƒëang nghe\n",
            "Cau da xu ly: 892020 v·∫´n ƒëang nghe\n",
            "Cau chua xu ly: 18/12/2021 nh·∫π nh√†ng v√† s√¢u l·∫Øng\n",
            "Cau da xu ly: 18122021 nh·∫π nh√†ng v√† s√¢u l·∫Øng\n",
            "Cau chua xu ly: c√°c b·∫°n c√≥ th·ªÉ ban ph√°t cho m√¨nh 1 like fanpage ƒë∆∞·ª£c kh√¥ng ? b·ªçn m√¨nh mong mu·ªën th√†nh l·∫≠p clb th·ªÉ thao ƒëi·ªán t·ª≠ n√™n th·ª±c s·ª± c·∫ßn like. c√°c b·∫°n gi√∫p m√¨nh v·ªõi ! m√¨nh c·∫£m ∆°n c√°c b·∫°n link : http://www.facebook.com/escvnua/\n",
            "Cau da xu ly: c√°c b·∫°n c√≥ th·ªÉ ban ph√°t cho m√¨nh 1 like fanpage ƒë∆∞·ª£c kh√¥ng ? b·ªçn m√¨nh mong mu·ªën th√†nh l·∫≠p clb th·ªÉ thao ƒëi·ªán t·ª≠ n√™n th·ª±c s·ª± c·∫ßn like. c√°c b·∫°n gi√∫p m√¨nh v·ªõi ! m√¨nh c·∫£m ∆°n c√°c b·∫°n link : http:www.facebook.comescvnua\n",
            "Cau chua xu ly: 5/5/2020\n",
            "Cau da xu ly: 552020\n",
            "Cau chua xu ly: 25/2/2021\n",
            "Cau da xu ly: 2522021\n",
            "Cau chua xu ly: 3 .00 am ng√†y 5/1/2020 . v·ª´a nghe nh·∫°c v·ª´a √¥n thi ƒë·ªÉ qua k·ª≥ thi ·ªü b√°ch khoa n√® .\n",
            "Cau da xu ly: 3 .00 am ng√†y 512020 . v·ª´a nghe nh·∫°c v·ª´a √¥n thi ƒë·ªÉ qua k·ª≥ thi ·ªü b√°ch khoa n√® .\n",
            "Cau chua xu ly: ch√∫c a s·ª©c kho·∫ª b√†i n√†y hay v√† √Ω nghƒ©a l·∫Øm\n",
            "Cau da xu ly: ch√∫c a s·ª©c kh·ªèe b√†i n√†y hay v√† √Ω nghƒ©a l·∫Øm\n",
            "Cau chua xu ly: amrabat ƒë√° hay th·∫ø nh·ªù , ti·ªÅn ki·∫øn t·∫°o cho ƒë·ªôi b·∫°n ghi b√†n lu√¥n haha. qu√° ch√°yyyy\n",
            "Cau da xu ly: amrabat ƒë√° hay th·∫ø nh·ªù , ti·ªÅn ki·∫øn t·∫°o cho ƒë·ªôi b·∫°n ghi b√†n lu√¥n haha. qu√° cha√Ωyyy\n",
            "Cau chua xu ly: ad cho xin link mp3 v·ªõi ·∫° , thks @@ nghi·ªán r·ªìiii\n",
            "Cau da xu ly: ad cho xin link mp3 v·ªõi ·∫° , thks @@ nghi·ªán r√¥√¨ii\n",
            "Cau chua xu ly: 3/2/2021- c·∫ßn th∆°\n",
            "Cau da xu ly: 322021- c·∫ßn th∆°\n",
            "Cau chua xu ly: hy v·ªçng sau n√†y tu·ªïi 30 m√¨nh s·∫Ω h·∫°nh ph√∫c c√πng lkh 28/9/2023 üéâ\n",
            "Cau da xu ly: hy v·ªçng sau n√†y tu·ªïi 30 m√¨nh s·∫Ω h·∫°nh ph√∫c c√πng lkh 2892023 üéâ\n",
            "Cau chua xu ly: 17/10/2020. team 9x\n",
            "Cau da xu ly: 17102020. team 9x\n",
            "Cau chua xu ly: 20/5/2021 2006 cafe google th∆∞∆°ng em nh√¨u üòò nh·ªõ em l·∫Øm c√¥ g√°i d·ªÖ th∆∞∆°ng xinh ƒë·∫πp gi·ªù em ·ªü ƒë√¢u ü•∫\n",
            "Cau da xu ly: 2052021 2006 cafe google th∆∞∆°ng em nh√¨u üòò nh·ªõ em l·∫Øm c√¥ g√°i d·ªÖ th∆∞∆°ng xinh ƒë·∫πp gi·ªù em ·ªü ƒë√¢u ü•∫\n",
            "Cau chua xu ly: ng√†y x∆∞a b√™n nhau v√¥ h·ªìn nhi√™n v√† th∆° ng√¢y ƒë√¥i m·∫Øt ng√†y n√†o n·ªìng ·∫•m r√µ ƒë√£ xa xƒÉm m·ªói khi m√¨nh b√™n nhau anh th∆∞·ªùng h√¥n l√™n ƒë√¥i m·∫Øt c·ªßa em v√† t·ª´ khi em ra ƒëi ƒë√¥i m·∫Øt ƒë√≥ ƒë√£ kh√¥ng thu·ªôc v·ªÅ anh n·ªØa bh ƒë√£ l√† c·ªßa ng∆∞·ªùi kh√°c r·ªìi anh th√¨ v·∫´n th·∫ø v·∫´n y√™u em m√£i m√£i √¢m th·∫ßm v√† lu√¥n d√µi theo em cho d√π em ƒë√£ b√™n ng∆∞·ªùi kh√°c y√™u em su·ªët ƒë·ªùi c√≥ ph·∫£i anh ng·ªëc qu√° k0 khi c·ª© ·∫£o t∆∞·ªüng r·∫±ng em c√≤n b√™n anh m√£i nh·ªØng t·ªânh gi·∫•c l·∫°i th√¨ h√≥a ra anh ch·ªâ n·∫±m m∆° 3/5/0202\n",
            "Cau da xu ly: ng√†y x∆∞a b√™n nhau v√¥ h·ªìn nhi√™n v√† th∆° ng√¢y ƒë√¥i m·∫Øt ng√†y n√†o n·ªìng ·∫•m r√µ ƒë√£ xa xƒÉm m·ªói khi m√¨nh b√™n nhau anh th∆∞·ªùng h√¥n l√™n ƒë√¥i m·∫Øt c·ªßa em v√† t·ª´ khi em ra ƒëi ƒë√¥i m·∫Øt ƒë√≥ ƒë√£ kh√¥ng thu·ªôc v·ªÅ anh n·ªØa bh ƒë√£ l√† c·ªßa ng∆∞·ªùi kh√°c r·ªìi anh th√¨ v·∫´n th·∫ø v·∫´n y√™u em m√£i m√£i √¢m th·∫ßm v√† lu√¥n d√µi theo em cho d√π em ƒë√£ b√™n ng∆∞·ªùi kh√°c y√™u em su·ªët ƒë·ªùi c√≥ ph·∫£i anh ng·ªëc qu√° k0 khi c·ª© ·∫£o t∆∞·ªüng r·∫±ng em c√≤n b√™n anh m√£i nh·ªØng t·ªânh gi·∫•c l·∫°i th√¨ h√≥a ra anh ch·ªâ n·∫±m m∆° 350202\n",
            "Cau chua xu ly: your music playlist is very good https://www.youtube.com/watch?v=loacel4inxq\n",
            "Cau da xu ly: your music playlist is very good https:www.youtube.comwatch?v=loacel4inxq\n",
            "Cau chua xu ly: 23/2/2021 - ƒë√£ gh√© thƒÉm\n",
            "Cau da xu ly: 2322021 - ƒë√£ gh√© thƒÉm\n",
            "Cau chua xu ly: ko c√≥ a tr∆∞·ªüng th√¨ may l√† ho√†\n",
            "Cau da xu ly: ko c√≥ a tr∆∞·ªüng th√¨ may l√† h√≤a\n",
            "Cau chua xu ly: 13/8/2020\n",
            "Cau da xu ly: 1382020\n",
            "Cau chua xu ly: 6/1/2021\n",
            "Cau da xu ly: 612021\n",
            "Cau chua xu ly: th·ªùi √Ω nguy·ªÖn ƒë·ª©c chung l√† ƒë·∫°i u√Ω üò¢\n",
            "Cau da xu ly: th·ªùi √Ω nguy·ªÖn ƒë·ª©c chung l√† ƒë·∫°i √∫y üò¢\n",
            "Cau chua xu ly: q√∫a hay.\n",
            "Cau da xu ly: qu√° hay.\n",
            "Cau chua xu ly: 20/11/202\n",
            "Cau da xu ly: 2011202\n",
            "Cau chua xu ly: 3 s·ªçc ƒë·ªè n·ªÅn v√†ng v·∫´n mang phong thu·ª∑ kh√¥ng t·ªët cho c√°c th·ªß m√¥n m√†...t·ª´ m√πa world cup t·ªõi gi·ªù lu√¥n\n",
            "Cau da xu ly: 3 s·ªçc ƒë·ªè n·ªÅn v√†ng v·∫´n mang phong th·ªßy kh√¥ng t·ªët cho c√°c th·ªß m√¥n m√†...t·ª´ m√πa world cup t·ªõi gi·ªù lu√¥n\n",
            "Cau chua xu ly: s√†i g√≤n chi·ªÅu m∆∞a th·ª© 7 ng√†y 11/05/2019\n",
            "Cau da xu ly: s√†i g√≤n chi·ªÅu m∆∞a th·ª© 7 ng√†y 11052019\n",
            "Cau chua xu ly: nghe nh·ªØng b√†i t√¨nh ca nh·∫π nh√†ng r·ªìi th√¨ c√πng nghe nh·ªØng giai ƒëi·ªáu t∆∞∆°i vui, r·ªôn r√†ng v·ªÅ t√¨nh y√™u nh√© ^^ https://www.youtube.com/watch?v=ghaechocwxq\n",
            "Cau da xu ly: nghe nh·ªØng b√†i t√¨nh ca nh·∫π nh√†ng r·ªìi th√¨ c√πng nghe nh·ªØng giai ƒëi·ªáu t∆∞∆°i vui, r·ªôn r√†ng v·ªÅ t√¨nh y√™u nh√© ^^ https:www.youtube.comwatch?v=ghaechocwxq\n",
            "Cau chua xu ly: mu b√¢y gi·ªù ch·ªâ l√† m·ªôt c√°i kho ƒëi·ªÉm kh·ªïng l·ªì m√† b·∫•t c·ª© ƒë·ªôi n√†o c≈©ng c√≥ th·ªÉ l·∫•y 3 ƒëi·ªÉm. n√≥i chung l√† m·ªôt c√°i ƒë·ªôi kh√¥ng c√≤n ai s·ª£ n·ª≠a. ƒë√° th√¨ ngu m√† ng√¥i sao ƒëi h·∫ßu to√† th√¨ nhi·ªÅu\n",
            "Cau da xu ly: mu b√¢y gi·ªù ch·ªâ l√† m·ªôt c√°i kho ƒëi·ªÉm kh·ªïng l·ªì m√† b·∫•t c·ª© ƒë·ªôi n√†o c≈©ng c√≥ th·ªÉ l·∫•y 3 ƒëi·ªÉm. n√≥i chung l√† m·ªôt c√°i ƒë·ªôi kh√¥ng c√≤n ai s·ª£ n·ª≠a. ƒë√° th√¨ ngu m√† ng√¥i sao ƒëi h·∫ßu t√≤a th√¨ nhi·ªÅu\n",
            "Cau chua xu ly: m·ªôt bu·ªïi tr∆∞a h√® ƒë·∫ßy gi√≥..... nghe th·∫≠t hay..... \"nghe n√†y ai ∆°i\" thank you admin =)). #05/05/2019\n",
            "Cau da xu ly: m·ªôt bu·ªïi tr∆∞a h√® ƒë·∫ßy gi√≥..... nghe th·∫≠t hay..... \"nghe n√†y ai ∆°i\" thank you admin =)). #05052019\n",
            "Cau chua xu ly: 15/2/2021\n",
            "Cau da xu ly: 1522021\n",
            "Cau chua xu ly: 1/1/2020...\n",
            "Cau da xu ly: 112020...\n",
            "Cau chua xu ly: t∆∞·ªüng nhu·ªôm ƒë·ªè tr·ªùi √¢u ch·ª© ho√° ra l√† mang nh·ª•c ·ªü tr·ªùi √¢u √†\n",
            "Cau da xu ly: t∆∞·ªüng nhu·ªôm ƒë·ªè tr·ªùi √¢u ch·ª© h√≥a ra l√† mang nh·ª•c ·ªü tr·ªùi √¢u √†\n",
            "Cau chua xu ly: 7/11 nh·ªõ ƒë·∫øn wanbi tu·∫•n anh.\n",
            "Cau da xu ly: 711 nh·ªõ ƒë·∫øn wanbi tu·∫•n anh.\n",
            "Cau chua xu ly: c√°m ∆°n. onana d·∫•t nhi·ªÅu.ƒë·∫∑c v·ª• ƒë∆∞·ª£c ph·∫£i ƒë·∫øn ƒë·ªÉ hu·ª∑ di·ªát tenhag\n",
            "Cau da xu ly: c√°m ∆°n. onana d·∫•t nhi·ªÅu.ƒë·∫∑c v·ª• ƒë∆∞·ª£c ph·∫£i ƒë·∫øn ƒë·ªÉ h·ªßy di·ªát tenhag\n",
            "Cau chua xu ly: laÃÄm sao ƒë√™Ãâ ki√™ÃÅm 1000$ t∆∞ÃÄ beegroud https://www.youtube.com/watch?v=x8mwtst-fao\n",
            "Cau da xu ly: laÃÄm sao ƒë√™Ãâ ki√™ÃÅm 1000$ t∆∞ÃÄ beegroud https:www.youtube.comwatch?v=x8mwtst-fao\n",
            "Cau chua xu ly: *hay l·∫Øm b·∫°n. cho m√¨nh h·ªèi b·∫°n l√†m video b·∫±ng app g√¨ v·∫≠y ·∫°? m√¨nh kh√¥ng bi·∫øt g·ªçi l√† g√¨ nh∆∞ng t·∫°m g·ªçi l√† animation l√∫c ƒë·∫ßu ƒë·∫πp ph·∫øt. to√†n video nh√¨n c≈©ng h√†i ho√† d·ªÖ ch·ªãu. ƒë·∫πp ƒë√≥ b·∫°n!*\n",
            "Cau da xu ly: *hay l·∫Øm b·∫°n. cho m√¨nh h·ªèi b·∫°n l√†m video b·∫±ng app g√¨ v·∫≠y ·∫°? m√¨nh kh√¥ng bi·∫øt g·ªçi l√† g√¨ nh∆∞ng t·∫°m g·ªçi l√† animation l√∫c ƒë·∫ßu ƒë·∫πp ph·∫øt. to√†n video nh√¨n c≈©ng h√†i h√≤a d·ªÖ ch·ªãu. ƒë·∫πp ƒë√≥ b·∫°n!*\n",
            "Cau chua xu ly: h√¥m nay t·ªõ nghe nhe27/9/2020\n",
            "Cau da xu ly: h√¥m nay t·ªõ nghe nhe2792020\n",
            "Cau chua xu ly: 8x v·∫´n c·ª© l√† chu·∫©n nh·∫•t... ƒë·∫πp nh·∫•t v·∫´n l√† 8x... 29/11/2020\n",
            "Cau da xu ly: 8x v·∫´n c·ª© l√† chu·∫©n nh·∫•t... ƒë·∫πp nh·∫•t v·∫´n l√† 8x... 29112020\n",
            "Cau chua xu ly: bao nhi√™u m√πa gi·∫£ mu to√†n th·ªß m√¥n gi·ªèii. tr·ª´ onana\n",
            "Cau da xu ly: bao nhi√™u m√πa gi·∫£ mu to√†n th·ªß m√¥n gio·ªâi. tr·ª´ onana\n",
            "Cau chua xu ly: 10h20 ng√†y 04/04/2021 8x c√≥ ai ƒëang ng·ªìi nghe nh∆∞ m√¨nh ko\n",
            "Cau da xu ly: 10h20 ng√†y 04042021 8x c√≥ ai ƒëang ng·ªìi nghe nh∆∞ m√¨nh ko\n",
            "Cau chua xu ly: n·∫øu l·∫•y t√™n dv ƒë·ªÉ ph·ª• ho·∫° nh√¢n v·∫≠t th√¨ vai ph·∫£n di·ªán l√† ph√†n thi·∫øu ho√†ng ch·ª© ƒë√¢u ph·∫£i ng√¥ ng·∫°n t·ªï\n",
            "Cau da xu ly: n·∫øu l·∫•y t√™n dv ƒë·ªÉ ph·ª• h·ªça nh√¢n v·∫≠t th√¨ vai ph·∫£n di·ªán l√† ph√†n thi·∫øu ho√†ng ch·ª© ƒë√¢u ph·∫£i ng√¥ ng·∫°n t·ªï\n",
            "Cau chua xu ly: em ƒë√£ r·ªùi xa anh...ch·∫≥ng k·ªãp n√≠uuuuuuuuuuuuuuuu\n",
            "Cau da xu ly: em ƒë√£ r·ªùi xa anh...ch·∫≥ng k·ªãp ni√∫uuuuuuuuuuuuuuu\n",
            "Cau chua xu ly: 4/4/2021 nghe \" ƒë√¥i m·∫Øt \" b·ªóng nhi√™n tay l·∫°i n·ªïi da g√†....nh·ªõ v·ªÅ ch√†ng ca sƒ© tr·∫ª t√†i nƒÉng.....\n",
            "Cau da xu ly: 442021 nghe \" ƒë√¥i m·∫Øt \" b·ªóng nhi√™n tay l·∫°i n·ªïi da g√†....nh·ªõ v·ªÅ ch√†ng ca sƒ© tr·∫ª t√†i nƒÉng.....\n",
            "Cau chua xu ly: hy v·ªçng sau n√†y tu·ªïi 30 m√¨nh s·∫Ω h·∫°nh ph√∫c c√πng lkh ‚ù§‚ù§ 28/9/2023 üéâ\n",
            "Cau da xu ly: hy v·ªçng sau n√†y tu·ªïi 30 m√¨nh s·∫Ω h·∫°nh ph√∫c c√πng lkh ‚ù§‚ù§ 2892023 üéâ\n",
            "Cau chua xu ly: 4:47 16/06/2020 692 like 0 dislike\n",
            "Cau da xu ly: 4:47 16062020 692 like 0 dislike\n",
            "Cau chua xu ly: 03/7/1989, gi·ªù ƒë√£ c√≥ 2 ƒë·ª©a con v√† nh·ªØng b√†i h√°t n√†y l√† m·ªôt th·ªùi gian ƒë·∫πp nh·∫•t, vi·∫øt th∆∞ t·ªè t√¨nh v·ªõi b·∫°n h·ªçc, gi·∫≠n h·ªùn vu v∆°, gi·ªù th√¨...\n",
            "Cau da xu ly: 0371989, gi·ªù ƒë√£ c√≥ 2 ƒë·ª©a con v√† nh·ªØng b√†i h√°t n√†y l√† m·ªôt th·ªùi gian ƒë·∫πp nh·∫•t, vi·∫øt th∆∞ t·ªè t√¨nh v·ªõi b·∫°n h·ªçc, gi·∫≠n h·ªùn vu v∆°, gi·ªù th√¨...\n",
            "Cau chua xu ly: 3/1/2020\n",
            "Cau da xu ly: 312020\n",
            "Cau chua xu ly: th·∫±ng ohaha t∆∞·ªüng ch·ªâ c·ª•t tay, ho√° ra gi·ªù c·ª•t c·∫£ ch√¢n, ho√† nh·∫≠p vs g√°nh xi·∫øc nhanh qu√° qu·∫£ th·ª© 3 icardi n√≥ l·ª° ƒë√† r·ªìi v·∫´n ƒë·ªï ng∆∞·ªùi cho n√≥ b·∫•m qua d√π kho·∫£ng c√°ch ko qu√° g·∫ßn, ph√°n ƒëo√°n t·ªá vl lu√¥n\n",
            "Cau da xu ly: th·∫±ng ohaha t∆∞·ªüng ch·ªâ c·ª•t tay, h√≥a ra gi·ªù c·ª•t c·∫£ ch√¢n, h√≤a nh·∫≠p vs g√°nh xi·∫øc nhanh qu√° qu·∫£ th·ª© 3 icardi n√≥ l·ª° ƒë√† r·ªìi v·∫´n ƒë·ªï ng∆∞·ªùi cho n√≥ b·∫•m qua d√π kho·∫£ng c√°ch ko qu√° g·∫ßn, ph√°n ƒëo√°n t·ªá vl lu√¥n\n",
            "Cau chua xu ly: ai c·ª©u r4 ƒëi qua m·ªèur ƒëi r4 ∆°i\n",
            "Cau da xu ly: ai c·ª©u r4 ƒëi qua mo·ªßr ƒëi r4 ∆°i\n",
            "Cau chua xu ly: q√∫a tuy·ªát v·ªùi cho m·ªôt list nh·∫°c to√†n nh·ªØng b√†i hay v√† kh√¥ng qu·∫£ng c√°o <3 c·∫£m ∆°n k√™nh v√¨ t·∫•t c·∫£ ^^\n",
            "Cau da xu ly: qu√° tuy·ªát v·ªùi cho m·ªôt list nh·∫°c to√†n nh·ªØng b√†i hay v√† kh√¥ng qu·∫£ng c√°o <3 c·∫£m ∆°n k√™nh v√¨ t·∫•t c·∫£ ^^\n",
            "Cau chua xu ly: dao hi·ªáu b·∫£o long c√≥ b√°n ·ªü qu·∫≠n 5 tphcm 1 tri·ªáu/c√¢y dao y chang trong phim lu√¥n ch·ª© kh√¥ng m·∫Øc nh∆∞ phim . c√≤n m√≥n ƒÉn th√¨ nh√† h√†ng qu·∫≠n 5 n·∫•u c≈©ng y chang phim lu√¥n .\n",
            "Cau da xu ly: dao hi·ªáu b·∫£o long c√≥ b√°n ·ªü qu·∫≠n 5 tphcm 1 tri·ªáuc√¢y dao y chang trong phim lu√¥n ch·ª© kh√¥ng m·∫Øc nh∆∞ phim . c√≤n m√≥n ƒÉn th√¨ nh√† h√†ng qu·∫≠n 5 n·∫•u c≈©ng y chang phim lu√¥n .\n",
            "Cau chua xu ly: h√£y th∆∞∆°ng tui share v√† like th·∫≠t nhi·ªÅuuu v√¨ s·∫Øp t·ªõi s·∫Ω c√≥ m·ªôt b·∫•t ng·ªù kh√°c version chill h∆°n t√≠ x√≠u cho t·∫•t c·∫£ c√°c b·∫°n n√®eee\n",
            "Cau da xu ly: h√£y th∆∞∆°ng tui share v√† like th·∫≠t nhi·ªÅuuu v√¨ s·∫Øp t·ªõi s·∫Ω c√≥ m·ªôt b·∫•t ng·ªù kh√°c version chill h∆°n t√≠ x√≠u cho t·∫•t c·∫£ c√°c b·∫°n ne√®ee\n",
            "Cau chua xu ly: ch·∫≥ng l·∫Ω em m√£i ch·∫≥ng thu·ªôc v·ªÅ anh √† =))) 21/06/2020\n",
            "Cau da xu ly: ch·∫≥ng l·∫Ω em m√£i ch·∫≥ng thu·ªôc v·ªÅ anh √† =))) 21062020\n",
            "Cau chua xu ly: m√™ l·∫Ømmm ·∫°aaaaaa\n",
            "Cau da xu ly: m√™ l·∫Ømmm a·∫°aaaaa\n",
            "Cau chua xu ly: c·∫£m ∆°n t·∫•t c·∫£ c√°c b·∫°n ƒë√£ th√≠ch b√†i h√°t c·ªßa m√¨nh h√£y ·ªßng h·ªô m√¨nh v√† b√†i h√°t n√†y nhi·ªÅu h∆°n n·ªØa ƒë·ªÉ c√≥ tu√¥i co√° ƒë·ªông l·ª±c ra b·∫£n rnb nghe c·ª±c chill lu√¥n n√†aa\\\n",
            "Cau da xu ly: c·∫£m ∆°n t·∫•t c·∫£ c√°c b·∫°n ƒë√£ th√≠ch b√†i h√°t c·ªßa m√¨nh h√£y ·ªßng h·ªô m√¨nh v√† b√†i h√°t n√†y nhi·ªÅu h∆°n n·ªØa ƒë·ªÉ c√≥ tu√¥i c√≥a ƒë·ªông l·ª±c ra b·∫£n rnb nghe c·ª±c chill lu√¥n na√†a\\\n",
            "Cau chua xu ly: kh√≥c cho tho·∫£ l√≤ng r·ªìi ng·ªß th√¥i\n",
            "Cau da xu ly: kh√≥c cho th·ªèa l√≤ng r·ªìi ng·ªß th√¥i\n",
            "Cau chua xu ly: ae n√†o bi·∫øt c√°ch t·∫£i nh·ªØng link nh·∫°c nh∆∞ v·∫≠y v·ªÅ laptop ko?! ch·ªâ m√¨nh nh√©eeeeeeeee\n",
            "Cau da xu ly: ae n√†o bi·∫øt c√°ch t·∫£i nh·ªØng link nh·∫°c nh∆∞ v·∫≠y v·ªÅ laptop ko?! ch·ªâ m√¨nh nhe√©eeeeeeee\n",
            "Cau chua xu ly: 24/08/2020\n",
            "Cau da xu ly: 24082020\n",
            "Cau chua xu ly: /12/2020 v·∫©n hay\n",
            "Cau da xu ly: 122020 v·∫©n hay\n",
            "Cau chua xu ly: 5/11/2020\n",
            "Cau da xu ly: 5112020\n",
            "Cau chua xu ly: 27/12 ƒë√¥i m·∫Øt bu·ªìn\n",
            "Cau da xu ly: 2712 ƒë√¥i m·∫Øt bu·ªìn\n",
            "Cau chua xu ly: 2/3.2021\n",
            "Cau da xu ly: 23.2021\n",
            "Cau chua xu ly: mu m√πa n√†y h·ªç ƒë√£ r·ªùi v√†o b·∫£ng ƒë·∫•u d·ªÖ th·ªü nh·∫•t nh·ªØng h·ªç ch∆°i qu√° t·ªá v√† nh·∫°t m∆° ho√† byen r·ªìi l·∫°i ƒë·ªÉ thua m·ªôt ƒë·ªôi b√≥ng t·∫ßm trung n·ªØa th·∫≠t s·ª± l√† qu√° t·ªá\n",
            "Cau da xu ly: mu m√πa n√†y h·ªç ƒë√£ r·ªùi v√†o b·∫£ng ƒë·∫•u d·ªÖ th·ªü nh·∫•t nh·ªØng h·ªç ch∆°i qu√° t·ªá v√† nh·∫°t m∆° h√≤a byen r·ªìi l·∫°i ƒë·ªÉ thua m·ªôt ƒë·ªôi b√≥ng t·∫ßm trung n·ªØa th·∫≠t s·ª± l√† qu√° t·ªá\n",
            "Cau chua xu ly: 14/2/2019\n",
            "Cau da xu ly: 1422019\n",
            "Cau chua xu ly: 14/12/2020\n",
            "Cau da xu ly: 14122020\n",
            "Cau chua xu ly: ch·ªã n√†o h√°t b√†i gi·∫•c m∆° tuy·∫øt tr·∫Øng d·ª° th·∫≠t. ph·∫£i chuy·ªÉn b√†i g·∫•p..... nghe ch·∫£yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy h·∫øt c·∫£ n∆∞·ªõc ra\n",
            "Cau da xu ly: ch·ªã n√†o h√°t b√†i gi·∫•c m∆° tuy·∫øt tr·∫Øng d·ª° th·∫≠t. ph·∫£i chuy·ªÉn b√†i g·∫•p..... nghe cha·ª∑yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy h·∫øt c·∫£ n∆∞·ªõc ra\n",
            "Cau chua xu ly: 11/2020 c√≥ ai c√≤n nghe kh√¥ng?\n",
            "Cau da xu ly: 112020 c√≥ ai c√≤n nghe kh√¥ng?\n",
            "Cau chua xu ly: 12/07/2020 ƒë√£ gh√© thƒÉm. c·ªõ sao up t·∫≠n 2 nƒÉm r·ªìi m√† m√¨nh m·ªõi th·∫•y, th·ª±c s·ª± th√≠ch nghe nh·ªØng b·∫£n nh·∫°c cover ng√†y x∆∞a hay nh∆∞ n√†y...\n",
            "Cau da xu ly: 12072020 ƒë√£ gh√© thƒÉm. c·ªõ sao up t·∫≠n 2 nƒÉm r·ªìi m√† m√¨nh m·ªõi th·∫•y, th·ª±c s·ª± th√≠ch nghe nh·ªØng b·∫£n nh·∫°c cover ng√†y x∆∞a hay nh∆∞ n√†y...\n",
            "Cau chua xu ly: 03/01/2021 anh tin m√¨nh ƒë√£ cho nhau m·ªôt k·ªâ ni·ªám r·ªìi m√£i m√£i anh kh√¥ng qu√™n ƒë∆∞·ª£c em!!!\n",
            "Cau da xu ly: 03012021 anh tin m√¨nh ƒë√£ cho nhau m·ªôt k·ªâ ni·ªám r·ªìi m√£i m√£i anh kh√¥ng qu√™n ƒë∆∞·ª£c em!!!\n",
            "Cau chua xu ly: v·∫•n ƒë·ªÅ k n·∫±m ·ªü mu m√† n·∫±m ·ªü ƒë·∫ß·ª´ tr·ªçc ten hag\n",
            "Cau da xu ly: v·∫•n ƒë·ªÅ k n·∫±m ·ªü mu m√† n·∫±m ·ªü ƒë·∫ß∆∞ tr·ªçc ten hag\n",
            "Cau chua xu ly: 09/09/2020, d√π c√≥ qua bao nhi√™u nƒÉm, bao nhi√™u b√†i h√°t m·ªõi ra m·∫Øt th√¨ m√¨nh v·∫´n ch·ªâ th√≠ch v√† mu·ªën nghe l·∫°i nh·ªØng b√†i h√°t ƒë√£ c≈© n√†y!\n",
            "Cau da xu ly: 09092020, d√π c√≥ qua bao nhi√™u nƒÉm, bao nhi√™u b√†i h√°t m·ªõi ra m·∫Øt th√¨ m√¨nh v·∫´n ch·ªâ th√≠ch v√† mu·ªën nghe l·∫°i nh·ªØng b√†i h√°t ƒë√£ c≈© n√†y!\n",
            "Cau chua xu ly: 10/4/2020\n",
            "Cau da xu ly: 1042020\n",
            "Cau chua xu ly: nghe b√†i n√†y th·∫•y nh·∫π nh√†ng, nghe t·ª´ l√∫c cu·ªëi nƒÉm l·ªõp 12. m√† gi·ªù ƒë√£ sv nƒÉm th·ª© 5 r·ªìi, nhanh th·∫≠t. 1 nƒÉm v·ªõi nhi·ªÅu s·ª± ki·ªán quan tr·ªçng!23/12/2018\n",
            "Cau da xu ly: nghe b√†i n√†y th·∫•y nh·∫π nh√†ng, nghe t·ª´ l√∫c cu·ªëi nƒÉm l·ªõp 12. m√† gi·ªù ƒë√£ sv nƒÉm th·ª© 5 r·ªìi, nhanh th·∫≠t. 1 nƒÉm v·ªõi nhi·ªÅu s·ª± ki·ªán quan tr·ªçng!23122018\n",
            "Cau chua xu ly: mo√° ƒë·ªïi m·ªôt c·∫ßu th·ªß ch∆°i tay hay b·∫±ng m·ªôt c·∫ßu th·ªß ch∆°i ch√¢n hay, m√† qu√™n m·∫•t r·∫±ng v·ªã tr√≠ th·ªß m√¥n c·∫ßn ch∆°i tay hay nhi·ªÅu h∆°n! m·∫π nh√¨n √¥ng th·ªß m√¥n h√£m th·∫≠t,\n",
            "Cau da xu ly: m√≥a ƒë·ªïi m·ªôt c·∫ßu th·ªß ch∆°i tay hay b·∫±ng m·ªôt c·∫ßu th·ªß ch∆°i ch√¢n hay, m√† qu√™n m·∫•t r·∫±ng v·ªã tr√≠ th·ªß m√¥n c·∫ßn ch∆°i tay hay nhi·ªÅu h∆°n! m·∫π nh√¨n √¥ng th·ªß m√¥n h√£m th·∫≠t,\n",
            "Cau chua xu ly: mu·ªën ronaldo ƒëi ch·ªâ v√¨ s·ª£ c·∫≠u ta to·∫£ s√°ng\n",
            "Cau da xu ly: mu·ªën ronaldo ƒëi ch·ªâ v√¨ s·ª£ c·∫≠u ta t·ªèa s√°ng\n",
            "Cau chua xu ly: 4,2n like, 19 dislike 19/6/2020\n",
            "Cau da xu ly: 4,2n like, 19 dislike 1962020\n",
            "Cau chua xu ly: th·∫±ng ku hojlun n√†y ƒë√° dc ƒë·∫•y ph√°t tri·ªÉn t·ªët th√¨ n√≥ c√≤n bi·∫øn ho√° h∆°n haland ·ªü kh·∫£ nƒÉng k√©o b√≥ng v√† tgia l·ªëi ch∆°i\n",
            "Cau da xu ly: th·∫±ng ku hojlun n√†y ƒë√° dc ƒë·∫•y ph√°t tri·ªÉn t·ªët th√¨ n√≥ c√≤n bi·∫øn h√≥a h∆°n haland ·ªü kh·∫£ nƒÉng k√©o b√≥ng v√† tgia l·ªëi ch∆°i\n",
            "Cau chua xu ly: 7/6/2020\n",
            "Cau da xu ly: 762020\n",
            "Cau chua xu ly: 19/4/2020\n",
            "Cau da xu ly: 1942020\n",
            "Cau chua xu ly: ng√¥ kh√¥ng l√™n tr·ªùi xu·ªëng bi·ªÉn 72 ph√©p bi·∫øn ho√° kh√¥ng d√πng l·∫°i d√πng √¥ haha. d√πng ch√¢n gi·ªèi ƒë√≥ l√† t√¥i ph·∫£i c√¥ng nh·∫≠n. nh∆∞ng ch·ªâ gi·ªèi d√πng ch√¢n chuy·ªÅn b√≥ng cho ƒë·ªëi th·ªß m√† th√¥i\n",
            "Cau da xu ly: ng√¥ kh√¥ng l√™n tr·ªùi xu·ªëng bi·ªÉn 72 ph√©p bi·∫øn h√≥a kh√¥ng d√πng l·∫°i d√πng √¥ haha. d√πng ch√¢n gi·ªèi ƒë√≥ l√† t√¥i ph·∫£i c√¥ng nh·∫≠n. nh∆∞ng ch·ªâ gi·ªèi d√πng ch√¢n chuy·ªÅn b√≥ng cho ƒë·ªëi th·ªß m√† th√¥i\n",
            "Cau chua xu ly: 19/2/2023\n",
            "Cau da xu ly: 1922023\n",
            "Cau chua xu ly: siu nh√¢n ho·∫£ long k√¨a ae\n",
            "Cau da xu ly: siu nh√¢n h·ªèa long k√¨a ae\n",
            "Cau chua xu ly: d√†n h·∫≠u v·ªá v√† th·ªß m√¥n mu xu·∫•t s·∫Øc t·∫∑ng qu√† cho ƒë·ªëi ph∆∞∆°ng d√†n c√¥ng r√∫t v·ªÅ sao k·ªãp. d√π r·∫•t th√≠ch mu, nh∆∞ng t√¥i ƒëo√°n mu gi·∫£i n√†o c≈©ng ch·ªâ ƒëi 5/10 ƒëo·∫°n ƒë∆∞·ªùng\n",
            "Cau da xu ly: d√†n h·∫≠u v·ªá v√† th·ªß m√¥n mu xu·∫•t s·∫Øc t·∫∑ng qu√† cho ƒë·ªëi ph∆∞∆°ng d√†n c√¥ng r√∫t v·ªÅ sao k·ªãp. d√π r·∫•t th√≠ch mu, nh∆∞ng t√¥i ƒëo√°n mu gi·∫£i n√†o c≈©ng ch·ªâ ƒëi 510 ƒëo·∫°n ƒë∆∞·ªùng\n",
            "Cau chua xu ly: 13/10/2020 v·∫´n hay\n",
            "Cau da xu ly: 13102020 v·∫´n hay\n",
            "Cau chua xu ly: e=h·ªçc/‚Ç´\n",
            "Cau da xu ly: e=h·ªçc‚Ç´\n",
            "Cau chua xu ly: clip nhu cai lol, am luong giam han xuong 1/3\n",
            "Cau da xu ly: clip nhu cai lol, am luong giam han xuong 13\n",
            "Cau chua xu ly: cho em xin h·ª£p √¢m v·ªõi ·∫°aaaaa\n",
            "Cau da xu ly: cho em xin h·ª£p √¢m v·ªõi a·∫°aaaa\n",
            "Cau chua xu ly: 2/3/2021 c√≤n nghe..\n",
            "Cau da xu ly: 232021 c√≤n nghe..\n",
            "Cau chua xu ly: 16/04/2021.\n",
            "Cau da xu ly: 16042021.\n",
            "Cau chua xu ly: th·∫£m ho·∫° onanaa tuy·ªát cmn v·ªùi\n",
            "Cau da xu ly: th·∫£m h·ªça onanaa tuy·ªát cmn v·ªùi\n",
            "Cau chua xu ly: 18/02/2021\n",
            "Cau da xu ly: 18022021\n",
            "Cau chua xu ly: 16/6 nay sinh nh·∫≠t tui n√® , c≈©ng m·ªõi chia tay xong !!! m·ªçi ng∆∞·ªùi ch√∫c t√¥i ƒëi :))\n",
            "Cau da xu ly: 166 nay sinh nh·∫≠t tui n√® , c≈©ng m·ªõi chia tay xong !!! m·ªçi ng∆∞·ªùi ch√∫c t√¥i ƒëi :))\n",
            "Cau chua xu ly: 31/3.2020 nghe l·∫°i\n",
            "Cau da xu ly: 313.2020 nghe l·∫°i\n",
            "Cau chua xu ly: anh ten n√≥i, khi trong ƒë·ªôi c√≥ 10 ng∆∞·ªùi ch·ªâ d√πng ch√¢n, t√¥i s·∫Ω bi·∫øn c·∫£ ƒë·ªôi ho√† tan v√† ƒëo√†n k·∫øt nh·∫•t c√≥ th·ªÉ b·∫±ng c√°ch ƒë·ªÉ th·ªß m√¥n d·∫ßn qu√™n 1 ƒëi·ªÅu r·∫±ng: anh ta c√≥ th·ªÉ d√πng tay ch∆°i b√≥ng :))\n",
            "Cau da xu ly: anh ten n√≥i, khi trong ƒë·ªôi c√≥ 10 ng∆∞·ªùi ch·ªâ d√πng ch√¢n, t√¥i s·∫Ω bi·∫øn c·∫£ ƒë·ªôi h√≤a tan v√† ƒëo√†n k·∫øt nh·∫•t c√≥ th·ªÉ b·∫±ng c√°ch ƒë·ªÉ th·ªß m√¥n d·∫ßn qu√™n 1 ƒëi·ªÅu r·∫±ng: anh ta c√≥ th·ªÉ d√πng tay ch∆°i b√≥ng :))\n",
            "Cau chua xu ly: 27/9/2019\n",
            "Cau da xu ly: 2792019\n",
            "Cau chua xu ly: sau chia tay ai c≈©ng kh√°c, t√¨m m·ªôt ng·ª´oi m·ªõi sau v·ª° n√°t. hay qu√° anh ∆°i. em th√≠ch nghe nh·∫°c c·ªßa anh l·∫Øm.\n",
            "Cau da xu ly: sau chia tay ai c≈©ng kh√°c, t√¨m m·ªôt ng∆∞√≤i m·ªõi sau v·ª° n√°t. hay qu√° anh ∆°i. em th√≠ch nghe nh·∫°c c·ªßa anh l·∫Øm.\n",
            "Cau chua xu ly: m·ªçi ng∆∞·ªùi ƒëƒÉng k√Ω k√™nh gi√∫p m√¨nh v·ªõi c√°m ∆°n mn! https://www.youtube.com/channel/ucq11lrio-cwcinfz4astnoq\n",
            "Cau da xu ly: m·ªçi ng∆∞·ªùi ƒëƒÉng k√Ω k√™nh gi√∫p m√¨nh v·ªõi c√°m ∆°n mn! https:www.youtube.comchannelucq11lrio-cwcinfz4astnoq\n",
            "Cau chua xu ly: c√°m ∆°n. onana d·∫•t nhi·ªÅu.ƒë·∫∑c v·ª• ƒë∆∞·ª£c ph·∫£i ƒë·∫øn ƒë·ªÉ hu·ª∑ di·ªát tenhag\n",
            "Cau da xu ly: c√°m ∆°n. onana d·∫•t nhi·ªÅu.ƒë·∫∑c v·ª• ƒë∆∞·ª£c ph·∫£i ƒë·∫øn ƒë·ªÉ h·ªßy di·ªát tenhag\n",
            "Cau chua xu ly: amrabat ƒë√° hay th·∫ø nh·ªù , ti·ªÅn ki·∫øn t·∫°o cho ƒë·ªôi b·∫°n ghi b√†n lu√¥n haha. qu√° ch√°yyyy\n",
            "Cau da xu ly: amrabat ƒë√° hay th·∫ø nh·ªù , ti·ªÅn ki·∫øn t·∫°o cho ƒë·ªôi b·∫°n ghi b√†n lu√¥n haha. qu√° cha√Ωyyy\n",
            "Cau chua xu ly: nh√¨n n√≥ ƒë√° m√† t·ª©c √≥i m√°uuu\n",
            "Cau da xu ly: nh√¨n n√≥ ƒë√° m√† t·ª©c √≥i ma√∫uu\n",
            "Cau chua xu ly: k√™nh n√†y ƒë√£ tha ho√°, n√≥ b√°n r·∫ª cho ƒë·ªìng ti·ªÅn. mu·ªën l√†m n√¥ l·ªá cho ƒë·ªìng ti·ªÅn nh∆∞ng c·ª© nh√¢n danh v√¨ t√≠nh ngh·ªá thu·∫≠t gi·∫£i tr√≠, t khinh ƒë·∫°o ƒë·ª©c gi·∫£ :)))\n",
            "Cau da xu ly: k√™nh n√†y ƒë√£ tha h√≥a, n√≥ b√°n r·∫ª cho ƒë·ªìng ti·ªÅn. mu·ªën l√†m n√¥ l·ªá cho ƒë·ªìng ti·ªÅn nh∆∞ng c·ª© nh√¢n danh v√¨ t√≠nh ngh·ªá thu·∫≠t gi·∫£i tr√≠, t khinh ƒë·∫°o ƒë·ª©c gi·∫£ :)))\n",
            "Cau chua xu ly: ƒë√£ xem phim ƒë·∫•t r·ª´ng ph∆∞∆°ng nam. ncl c≈©ng ƒë∆∞·ª£c nh∆∞ng ch·ªâ 4/10 c·ªßa b·∫£n g·ªëc th√¥i\n",
            "Cau da xu ly: ƒë√£ xem phim ƒë·∫•t r·ª´ng ph∆∞∆°ng nam. ncl c≈©ng ƒë∆∞·ª£c nh∆∞ng ch·ªâ 410 c·ªßa b·∫£n g·ªëc th√¥i\n",
            "Cau chua xu ly: r√°c th·∫≠t s·ª± , n√†y ƒë√∫ng m·∫•y ng t√≤ m√≤ ƒëi xem ho·∫∑c ko c√≥ film g√¨ th√¨ xem, ch·ª© ch√°n ph√®o :d 1/10\n",
            "Cau da xu ly: r√°c th·∫≠t s·ª± , n√†y ƒë√∫ng m·∫•y ng t√≤ m√≤ ƒëi xem ho·∫∑c ko c√≥ film g√¨ th√¨ xem, ch·ª© ch√°n ph√®o :d 110\n",
            "Cau chua xu ly: n·∫øu nh√¨n v·ªÅ g√≥c ƒë·ªô vƒÉn ho√° th√¨ ƒë√¢y l√† m·ªôt ch·∫•t ƒë·ªôc.\n",
            "Cau da xu ly: n·∫øu nh√¨n v·ªÅ g√≥c ƒë·ªô vƒÉn h√≥a th√¨ ƒë√¢y l√† m·ªôt ch·∫•t ƒë·ªôc.\n",
            "Cau chua xu ly: phim cringe b·ªè mi·∫π\n",
            "Cau da xu ly: phim cringe b·ªè m·ªãe\n",
            "Cau chua xu ly: b√° d∆° x√¢m tr·ªï g·∫∑p ƒë·∫∑p ƒë≈©y tho·∫£ l th√∫i,h·ª£p h·ª£p\n",
            "Cau da xu ly: b√° d∆° x√¢m tr·ªï g·∫∑p ƒë·∫∑p ƒë≈©y th·ªèa l th√∫i,h·ª£p h·ª£p\n",
            "Cau chua xu ly: c√°ch gi·∫£i quy·∫øt m·ªçi v·ª• vi·ªác c·ªßa c√¥ng tr·∫ßn ; lu√¥n dƒ© ho√† vi qu√Ω; th·∫•u t√¨nh ng∆∞·ªùi ! r·∫•t tr√¢n qu√Ω t√≠nh c√°ch c·ªßa em ! r·∫•t th·∫≥ng th·∫Øn !\n",
            "Cau da xu ly: c√°ch gi·∫£i quy·∫øt m·ªçi v·ª• vi·ªác c·ªßa c√¥ng tr·∫ßn ; lu√¥n dƒ© h√≤a vi qu√Ω; th·∫•u t√¨nh ng∆∞·ªùi ! r·∫•t tr√¢n qu√Ω t√≠nh c√°ch c·ªßa em ! r·∫•t th·∫≥ng th·∫Øn !\n",
            "Cau chua xu ly: con q·ªßy c√°i th·∫•y g·ªõm!cho hk th√®m\n",
            "Cau da xu ly: con qu·ª∑ c√°i th·∫•y g·ªõm!cho hk th√®m\n",
            "Cau chua xu ly: m·∫πeeeeeeee n√≥ phim d·ªü vcl\n",
            "Cau da xu ly: me·∫πeeeeeee n√≥ phim d·ªü vcl\n",
            "Cau chua xu ly: cu·ªën v√£i √≤oooooooooooo\n",
            "Cau da xu ly: cu·ªën v√£i o√≤ooooooooooo\n",
            "Cau chua xu ly: ch√≤i oii ƒë·∫πp trzai qu√°aaa\n",
            "Cau da xu ly: ch√≤i oii ƒë·∫πp trzai qua√°aa\n",
            "Cau chua xu ly: cu·ªën v√£iii\n",
            "Cau da xu ly: cu·ªën vaƒ©ii\n",
            "Cau chua xu ly: q√∫a tuy·ªát v·ªùi\n",
            "Cau da xu ly: qu√° tuy·ªát v·ªùi\n",
            "Cau chua xu ly: cu·ªën v√£iii hay qu√°\n",
            "Cau da xu ly: cu·ªën vaƒ©ii hay qu√°\n",
            "Cau chua xu ly: 1000000000/10 ƒëi·ªÉm excellent\n",
            "Cau da xu ly: 100000000010 ƒëi·ªÉm excellent\n",
            "Cau chua xu ly: ng·∫ßu qu√°aa m√™ ch·ªã minh anh g√™\n",
            "Cau da xu ly: ng·∫ßu qua√°a m√™ ch·ªã minh anh g√™\n",
            "Cau chua xu ly: xinh m√† c√≤n gi·ªèii\n",
            "Cau da xu ly: xinh m√† c√≤n gio·ªâi\n",
            "Cau chua xu ly: nghe em n√†y h√πng bi·ªán m√† th·∫•y bu·ªìn, m·ªôt th·∫ø h·ªá ch·ªâ bi·∫øt ƒë·ªó l·ªói cho ng∆∞·ªùi kh√°c ƒë√£ b·∫Øt ƒë·∫ßu l·ªõn l√™n./.\n",
            "Cau da xu ly: nghe em n√†y h√πng bi·ªán m√† th·∫•y bu·ªìn, m·ªôt th·∫ø h·ªá ch·ªâ bi·∫øt ƒë·ªó l·ªói cho ng∆∞·ªùi kh√°c ƒë√£ b·∫Øt ƒë·∫ßu l·ªõn l√™n..\n",
            "Cau chua xu ly: -c∆∞·ªùi lao xu·ªëng s√¥ng- l√†m r·∫•t ƒë√∫ng ch·∫•t phim th·∫£m ho·∫°, √Ω t b·ªô phim l√† th·∫£m ho·∫° c·ªßa n·ªÅn ƒëi·ªán ·∫£nh\n",
            "Cau da xu ly: -c∆∞·ªùi lao xu·ªëng s√¥ng- l√†m r·∫•t ƒë√∫ng ch·∫•t phim th·∫£m h·ªça, √Ω t b·ªô phim l√† th·∫£m h·ªça c·ªßa n·ªÅn ƒëi·ªán ·∫£nh\n",
            "Cau chua xu ly: m√¨nh th·∫•y ho√° trang v√† k·ªπ x·∫£o ch∆∞a ƒë·∫°t v√¨ phim zombie d·ª±a nhi·ªÅu v√†o 2 th·ª© tr√™n =))) di·ªÖn xu·∫•t c≈©ng n·ª≠a v·ªùi =))) ch√™\n",
            "Cau da xu ly: m√¨nh th·∫•y h√≥a trang v√† k·ªπ x·∫£o ch∆∞a ƒë·∫°t v√¨ phim zombie d·ª±a nhi·ªÅu v√†o 2 th·ª© tr√™n =))) di·ªÖn xu·∫•t c≈©ng n·ª≠a v·ªùi =))) ch√™\n",
            "Cau chua xu ly: th·∫£m ho·∫° n·ªÅn ƒëi·ªán ·∫£nh n∆∞·ªõc nh√† n√≥i ri√™ng v√† th·∫ø gi·ªõi n√≥i chung :))\n",
            "Cau da xu ly: th·∫£m h·ªça n·ªÅn ƒëi·ªán ·∫£nh n∆∞·ªõc nh√† n√≥i ri√™ng v√† th·∫ø gi·ªõi n√≥i chung :))\n",
            "Cau chua xu ly: xem r·ªìi ph√≠ ti·ªÅn √≠ch :)))) mo√°\n",
            "Cau da xu ly: xem r·ªìi ph√≠ ti·ªÅn √≠ch :)))) m√≥a\n",
            "Cau chua xu ly: t·∫ßm n√†y th√¨ ƒëi l√†m phim truy·ªÅn h√¨nh ƒëi ch·ª© c√≥ phim ƒëi·ªán ·∫£nh n√†o c·ªßa vi·ªát nam hay ƒë√¢u :/\n",
            "Cau da xu ly: t·∫ßm n√†y th√¨ ƒëi l√†m phim truy·ªÅn h√¨nh ƒëi ch·ª© c√≥ phim ƒëi·ªán ·∫£nh n√†o c·ªßa vi·ªát nam hay ƒë√¢u :\n",
            "Cau chua xu ly: phim ng∆∞·ªùi vi·ªát th∆∞·ªùng kh√° t·ªá c√≤n phim n√†y th√¨ l√† th·∫•t b·∫°i c·ªßa t·∫°o ho√° ch√™ ƒëi\n",
            "Cau da xu ly: phim ng∆∞·ªùi vi·ªát th∆∞·ªùng kh√° t·ªá c√≤n phim n√†y th√¨ l√† th·∫•t b·∫°i c·ªßa t·∫°o h√≥a ch√™ ƒëi\n",
            "Cau chua xu ly: phim vn l√† phim hay nh·∫•t tg ai c√£i l·∫°i ch√™ phim vn l√† 3/\n",
            "Cau da xu ly: phim vn l√† phim hay nh·∫•t tg ai c√£i l·∫°i ch√™ phim vn l√† 3\n",
            "Cau chua xu ly: b√†i hay waa tr lun √≠iii ü§Ø\n",
            "Cau da xu ly: b√†i hay waa tr lun i√≠ii ü§Ø\n",
            "Cau chua xu ly: hay quaas w/n ∆°i\n",
            "Cau da xu ly: hay quaas wn ∆°i\n",
            "Cau chua xu ly: t·ªõ r·∫•t th√≠ch b√†i h√°t n√†y. th·ª±c s·ª± l√† n√≥ ƒë√£ ƒë·ªìng h√†nh c√πng t·ªõ v√† anh t·ªõ su·ªët th·ªùi thanh xu√¢n, nghe b√†i h√°t ·∫•y. t·ªõ c·∫£m th·∫•y nh∆∞ mk ko c√≤n b·ªã b·ªè r∆°i, ko b·ªã b·∫°o l·ª±c h·ªçc ƒë∆∞·ªùng nh∆∞ ng√†y x∆∞a nx. c·∫£m ∆°n w /n r·∫•t nhi·ªÅu nh√©! (>3<) „ÅÇ„Çä„Åå„Å®„ÅÜ\n",
            "Cau da xu ly: t·ªõ r·∫•t th√≠ch b√†i h√°t n√†y. th·ª±c s·ª± l√† n√≥ ƒë√£ ƒë·ªìng h√†nh c√πng t·ªõ v√† anh t·ªõ su·ªët th·ªùi thanh xu√¢n, nghe b√†i h√°t ·∫•y. t·ªõ c·∫£m th·∫•y nh∆∞ mk ko c√≤n b·ªã b·ªè r∆°i, ko b·ªã b·∫°o l·ª±c h·ªçc ƒë∆∞·ªùng nh∆∞ ng√†y x∆∞a nx. c·∫£m ∆°n w n r·∫•t nhi·ªÅu nh√©! (>3<) „ÅÇ„Çä„Åå„Å®„ÅÜ\n",
            "Cau chua xu ly: nghe rat chill cam on w/n ‚ù§\n",
            "Cau da xu ly: nghe rat chill cam on wn ‚ù§\n",
            "Cau chua xu ly: e xin l·ªùi c·ªßa ƒëo·∫°n ƒë·∫ßu ƒë·ªÉ cover ·∫° üíó hay qu√°aa\n",
            "Cau da xu ly: e xin l·ªùi c·ªßa ƒëo·∫°n ƒë·∫ßu ƒë·ªÉ cover ·∫° üíó hay qua√°a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BgffCFZe8ka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "48ad37c0-3716-410e-fc03-0b6985f478b3"
      },
      "source": [
        "text = u'This is a smiley face üòÇ'\n",
        "text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a smiley face üòÇ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFnYCXtDM7tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42c05af-a400-4776-8301-a3fe9aabcef8"
      },
      "source": [
        "!pip install spacymoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacymoji\n",
            "  Downloading spacymoji-3.1.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacymoji) (3.6.1)\n",
            "Collecting emoji<3.0,>=2.0 (from spacymoji)\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacymoji) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacymoji) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacymoji) (2.1.3)\n",
            "Installing collected packages: emoji, spacymoji\n",
            "Successfully installed emoji-2.9.0 spacymoji-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPj5H-HXN5ht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78fc640a-1e61-4468-ea73-45c1a8bc225c"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-24 03:12:49.645840: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-24 03:12:49.645911: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-24 03:12:49.660501: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-24 03:12:49.689317: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-24 03:12:52.898578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Owkp-ZNMq6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f34ed9ab-d0f6-4d5f-c198-f179f33aaf9f"
      },
      "source": [
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacymoji import Emoji\n",
        "\n",
        "nlp = en_core_web_sm.load()\n",
        "emoji = Emoji(nlp)\n",
        "nlp.add_pipe('emoji', first=True)\n",
        "\n",
        "doc = nlp(\"sale hay sela :))\")\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sale', 'hay', 'sela', ':))']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8AljlN8R4F6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9348e8-fbde-4932-fe58-41d3a26eb59a"
      },
      "source": [
        "doc = nlp(\"sale hay sela üòÇ th·∫≠t v√£i l·ªìn ha\")\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sale', 'hay', 'sela', 'üòÇ', 'th·∫≠t', 'v√£i', 'l·ªìn', 'ha']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpooafddW_s0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d1cbcc-eea9-4de2-8495-4071ac71dbda"
      },
      "source": [
        "doc[3]._.is_emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vncorenlp"
      ],
      "metadata": {
        "id": "mzd8neE8Jejx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce7fd0d-53aa-4cc1-f78b-488795296caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2023.11.17)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645932 sha256=00f3103b9bde171ab062a50d1f6df95ea06d0013ec7a7765a69bd75ca1f34681\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvqmOXGZPd2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58acac23-9268-441a-d350-f475ca5e5193"
      },
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"/content/drive/MyDrive/VnCoreNLP-1.2/VnCoreNLP-1.2/VnCoreNLP-1.2.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "word_segmented_text = rdrsegmenter.tokenize(\"sale hay sela :D th·∫≠t v√£i l·ªìn ha üòÇüòÇüòÇüòÇüòÇ\")\n",
        "word_segmented_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['sale', 'hay', 'sela', ':', 'D', 'th·∫≠t', 'v√£i', 'l·ªìn', 'ha', 'üòÇüòÇüòÇüòÇüòÇ']]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK7GdrRkT3yl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513a21b6-3190-4c01-9471-a4169d79d981"
      },
      "source": [
        "!pip install demoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting demoji\n",
            "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/42.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: demoji\n",
            "Successfully installed demoji-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdLqiFAjTqN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea11e466-ecb3-4504-f33d-1ec59b8bfa8d"
      },
      "source": [
        "import demoji\n",
        "demoji.download_codes()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-eb011a9810ad>:2: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
            "  demoji.download_codes()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsncdHBOUFSP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "276a4a53-8bbf-4ffb-cf43-402d39204943"
      },
      "source": [
        "text = \"sale hay sela :)))))))))))))) th·∫≠t v√£i l·ªìn ha\"\n",
        "re.sub(r'([A-Z])\\1+', lambda m: m.group(1), text, flags = re.IGNORECASE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sale hay sela :)))))))))))))) th·∫≠t v√£i l·ªìn ha'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4QrjEo4U86u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7a9f58-4ced-4e14-dc49-cc1c296699ae"
      },
      "source": [
        "demoji.findall(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea\n",
        "\n",
        "from underthesea import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHwc7L9PyhlA",
        "outputId": "cf4865d0-e40c-4684-ed52-2221ae894c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.11.17)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.2.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.10 underthesea-6.8.0 underthesea-core-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "teencode_df = pd.read_csv('/content/drive/MyDrive/Textual data/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
        "teencode_list = teencode_df['teencode'].to_list()\n",
        "map_list = teencode_df['map'].to_list()\n",
        "def searchTeencode(word):\n",
        "  try:\n",
        "    global teencode_count\n",
        "    index = teencode_list.index(word)\n",
        "    map_word = map_list[index]\n",
        "    teencode_count += 1\n",
        "    return map_word\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "rM4anXW0yiBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = '/content/drive/MyDrive/Textual data/vietnamese-stopwords-dash.txt'\n",
        "\n",
        "# features extraction\n",
        "with open(STOPWORDS, \"r\") as ins:\n",
        "    stopword = []\n",
        "    for line in ins:\n",
        "        stopword.append(line.strip('\\n'))\n",
        "\n",
        "def remove_stopwords(line):\n",
        "    global stopword_count\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "        if word in stopword:\n",
        "            stopword_count += 1\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "sQSiqaSbysBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_count = 0\n",
        "teencode_count =0\n",
        "def stopWords_Teencode(sentence):\n",
        "  lenn = 0\n",
        "  sentence = str(sentence)\n",
        "  #Tokenize\n",
        "  List_tokens = word_tokenize(sentence,format='text')\n",
        "  List_tokens = word_tokenize(List_tokens)\n",
        "\n",
        "  #Teencode\n",
        "  for tokens_idx, text_tokens in enumerate(List_tokens):\n",
        "    deteencoded = searchTeencode(text_tokens)\n",
        "    if (deteencoded != None):\n",
        "        List_tokens[tokens_idx] = deteencoded\n",
        "\n",
        "  deteencode_sentence = (\" \").join(List_tokens)\n",
        "\n",
        "  #Stopwords\n",
        "  tokens_without_sw = remove_stopwords(deteencode_sentence)\n",
        "\n",
        "  return tokens_without_sw"
      ],
      "metadata": {
        "id": "GQ62MKIJyw4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopword_count = 0\n",
        "teencode_count =0\n",
        "train['vi_review'] = train['vi_review'].apply(lambda x:stopWords_Teencode(x))"
      ],
      "metadata": {
        "id": "aD6OpA2RzoY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF__6Sobo7tz"
      },
      "source": [
        "# EXPORT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbfP4n2rhP37"
      },
      "source": [
        "train.to_csv('/content/drive/MyDrive/siu.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}